{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from rectools import Columns\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Reciprocal Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MRR = \\frac{1}{|Q|}\\sum_{i=1}^{|Q|}\\frac{1}{rank_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get KION dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "    'https://github.com/irsafilo/KION_DATASET/raw/'\n",
    "    'f69775be31fa5779907cf0a92ddedb70037fb5ae/data_original.zip'\n",
    ")\n",
    "\n",
    "req = requests.get(url, stream=True)\n",
    "\n",
    "with open('kion.zip', 'wb') as fd:\n",
    "    total_size_in_bytes = int(req.headers.get('Content-Length', 0))\n",
    "    progress_bar = tqdm(\n",
    "        desc='kion dataset download',\n",
    "        total=total_size_in_bytes,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "    )\n",
    "    for chunk in req.iter_content(chunk_size=2 ** 20):\n",
    "        progress_bar.update(len(chunk))\n",
    "        fd.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o kion.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv('data_original/interactions.csv')\n",
    "\n",
    "interactions.rename(\n",
    "    columns={\n",
    "        'track_id': Columns.Item,\n",
    "        'last_watch_dt': Columns.Datetime,\n",
    "        'total_dur': Columns.Weight,\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "interactions[Columns.Datetime] = pd.to_datetime(interactions[Columns.Datetime])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsample(users_count, top_k):\n",
    "    users = np.random.choice(\n",
    "        interactions[Columns.User].unique(), users_count, replace=False,\n",
    "    )\n",
    "    df = interactions[interactions[Columns.User].isin(users)].reset_index(\n",
    "        drop=True,\n",
    "    )\n",
    "    del df[Columns.Datetime], df[Columns.Weight], df['watched_pct']\n",
    "\n",
    "    recs = np.random.choice(df[Columns.Item], size=(users_count, top_k))\n",
    "    return df, users, recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr_naive(target, users, recs):\n",
    "    mrr = []\n",
    "    for i, user in enumerate(users):\n",
    "        hit_rank = 0\n",
    "        user_target = target[target[:, 0] == user][:, 1]\n",
    "        for rank, rec in enumerate(recs[i]):\n",
    "            if rec in user_target:\n",
    "                hit_rank = rank + 1\n",
    "                break\n",
    "        mrr.append(1 / hit_rank) if hit_rank else mrr.append(0)\n",
    "    return sum(mrr) / len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(cache=True, parallel=True)\n",
    "def mrr_numba(target, users, recs):\n",
    "    mrr = np.zeros(len(users))\n",
    "    for i in nb.prange(len(users)):\n",
    "        hit_rank = 0\n",
    "        user_target = target[target[:, 0] == users[i]][:, 1]\n",
    "        for rank in nb.prange(len(recs[i])):\n",
    "            if recs[i][rank] in user_target:\n",
    "                hit_rank = rank + 1\n",
    "                break\n",
    "        mrr[i] = 1 / hit_rank if hit_rank else 0\n",
    "    return mrr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr_pandas(df, users, recs, k):\n",
    "    df_recs = pd.DataFrame(\n",
    "        {\n",
    "            Columns.User: np.repeat(users, k),\n",
    "            Columns.Item: recs.ravel(),\n",
    "        },\n",
    "    )\n",
    "    df_recs[Columns.Rank] = df_recs.groupby(Columns.User).cumcount() + 1\n",
    "    df_recs = df.merge(\n",
    "        df_recs,\n",
    "        how='left',\n",
    "        left_on=Columns.UserItem,\n",
    "        right_on=Columns.UserItem,\n",
    "    )\n",
    "    hit_ranks = 1 / len(users) / df_recs.groupby(\n",
    "        Columns.User,\n",
    "    )[Columns.Rank].min()\n",
    "    return hit_ranks.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_counts = [100, 1000, 10000, 100000]\n",
    "top_ks = [10, 50, 100]\n",
    "algos = [mrr_naive, mrr_numba, mrr_pandas]\n",
    "params = list(itertools.product(users_counts, top_ks, algos))\n",
    "measurements = {\n",
    "    'users_count': [],\n",
    "    'top_k': [],\n",
    "    'algo': [],\n",
    "    'avg_time': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим корректность работы\n",
    "df, users, recs = generate_subsample(users_counts[0], top_ks[0])\n",
    "target = df.values\n",
    "mrr_values = [\n",
    "    mrr_naive(target, users, recs),\n",
    "    mrr_numba(target, users, recs),\n",
    "    mrr_pandas(df, users, recs, top_ks[0]),\n",
    "]\n",
    "\n",
    "for mrr_pair in itertools.combinations(mrr_values, 2):\n",
    "    np.testing.assert_almost_equal(mrr_pair[0], mrr_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_set in params:\n",
    "    users_count, top_k, algo = param_set\n",
    "    print(\n",
    "        'users_count: {users_count}, top_k: {top_k}, algo: {algo}'.format(\n",
    "            users_count=users_count,\n",
    "            top_k=top_k,\n",
    "            algo=algo.__name__,\n",
    "        ),\n",
    "    )\n",
    "    df, users, recs = generate_subsample(users_count, top_k)\n",
    "    target = df.values\n",
    "    if algo == mrr_naive:\n",
    "        runs = %timeit -o -n 3 -r 1 algo(target, users, recs)\n",
    "    elif algo == mrr_numba:\n",
    "        algo(target, users, recs)\n",
    "        runs = %timeit -o -n 3 -r 1 algo(target, users, recs)\n",
    "    else:\n",
    "        runs = %timeit -o -n 3 -r 1 algo(df, users, recs, top_k)\n",
    "    measurements['users_count'].append(str(users_count))\n",
    "    measurements['top_k'].append(str(top_k))\n",
    "    measurements['algo'].append(algo.__name__)\n",
    "    measurements['avg_time'].append(np.mean(runs.timings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_df = pd.DataFrame(measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pFound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$pFound@K = \\sum_{i=1}^{k}pLook[i]pRel[i]$\n",
    "$pLook[1] = 1$\n",
    "$pLook[i] = pLook[i-1](1 - pRel[i-1])(1 - pBreak)$\n",
    "$pBreak = 0.15$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По формуле распишем первые несколько значений $pLook$ при $(pBreak = 0.15)$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&pLook[1] = 1\\\\\n",
    "&pLook[2] = (1 - pRel[1]) \\cdot 0.85\\\\\n",
    "&pLook[3] = (1 - pRel[1]) \\cdot 0.85 \\cdot (1 - pRel[2]) \\cdot 0.85\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Заметим закономерность. Тогда $pLook[i]$ для $i \\neq 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&[1] \\quad pLook[i] = (1 - pRel[1]) \\cdot 0.85 \\cdot (1 - pRel[2]) \\cdot 0.85 \\cdots (1 - pRel[i - 1]) \\cdot 0.85\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_found(df, p_break=0.15, k=None):\n",
    "    df['i < k'] = True\n",
    "    # Создадим столбец с рангом результата\n",
    "    df[Columns.Rank] = df.groupby('qid').cumcount() + 1\n",
    "    if k is not None:\n",
    "        # Если задано значение k, то игнорируем записи, для которых rank > k\n",
    "        df['i < k'] = df[Columns.Rank] < k + 1\n",
    "    # Сделаем сдвиг внутри каждой группы для дальнейших расчетов\n",
    "    # Теперь в соседнем столбце для каждого pRel[i] будет значение pRel[i - 1]\n",
    "    df['p_rel[i-1]'] = df.groupby('qid')['p_rel[i]'].shift()\n",
    "    # Рассчитаем (1 - pRel[i - 1]) * (1 - pBreak)\n",
    "    df['(1-p_rel[i-1])*(1-p_break)'] = (1 - df['p_rel[i-1]']) * (1 - p_break)\n",
    "    # Рассчитаем pLook[i] по формуле [1]\n",
    "    df['p_look[i]'] = df.groupby('qid')['(1-p_rel[i-1])*(1-p_break)'].cumprod()\n",
    "    # Выполним условие pLook[1] = 1\n",
    "    df['p_look[i]'] = df['p_look[i]'].fillna(1)\n",
    "    # Рассчитаем pFound[i]\n",
    "    # Домножение на df['i < k'] позволяет учитывать только первые k результатов\n",
    "    df['p_found[i]'] = df['p_look[i]'] * df['p_rel[i]'] * df['i < k']\n",
    "    # Для каждого запроса суммируем значение pFound и берем среднее\n",
    "    return df.groupby('qid')['p_found[i]'].sum().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "Данные - https://yadi.sk/d/guqki4UI4hFlXQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o open_task.zip\n",
    "!unzip -o hidden_task.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['qid', 'url', 'p_rel[i]']\n",
    "open_df = pd.read_csv(\n",
    "    'open_task/qid_url_rating.tsv', sep='\\t', names=column_names,\n",
    ")\n",
    "hidden_df = pd.read_csv(\n",
    "    'qid_url_rating.tsv', sep='\\t', names=column_names,\n",
    ")\n",
    "merged_df = pd.concat([open_df, hidden_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'pFound на открытой части датасета: {0:.5f}'.format(\n",
    "        p_found(open_df),\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    'pFound на закрытой части датасета: {0:.5f}'.format(\n",
    "        p_found(hidden_df),\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    'pFound на всем датасете: {0:.5f}'.format(\n",
    "        p_found(merged_df),\n",
    "    ),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
