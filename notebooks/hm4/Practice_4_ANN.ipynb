{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a18cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import nmslib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rectools import Columns\n",
    "from rectools.dataset import Dataset\n",
    "from rectools.metrics import calc_metrics, MAP\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0964ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78173939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmAnn(object):\n",
    "    \"\"\"ANN model on top of user and item embeddings.\"\"\"\n",
    "    attrs_to_load = [\n",
    "        'user_embeddings',\n",
    "        'item_embeddings',\n",
    "        'interactions',\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dirname: str,\n",
    "        ef_construction: int = 256,\n",
    "        n_threads_construction: int = 4,\n",
    "        ef_search: int = 256,\n",
    "        m: int = 72,\n",
    "        user_column: str = 'user_id',\n",
    "        item_column: str = 'item_id',\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize an ANN model.\n",
    "\n",
    "        Args:\n",
    "            dirname: Directory with fixtures.\n",
    "            ef_construction: The size of the dynamic list\n",
    "                for the nearest neighbors during index construction.\n",
    "            n_threads_construction: Index construction parameter.\n",
    "            ef_search: The size of the dynamic list\n",
    "                for the nearest neighbors during search.\n",
    "            m: The number of bidirectional links.\n",
    "            user_column: The user column name.\n",
    "            item_column: The item column name.\n",
    "        \"\"\"\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.index = nmslib.init(\n",
    "            method='hnsw',\n",
    "            space='negdotprod',\n",
    "            data_type=nmslib.DataType.DENSE_VECTOR,\n",
    "        )\n",
    "        state = self._load(dirname)\n",
    "        self.user_embeddings = self._aug_zero(\n",
    "            state['user_embeddings'],\n",
    "        )\n",
    "        self.item_embeddings = self._aug_inner_product(\n",
    "            state['item_embeddings'],\n",
    "        )\n",
    "        self.interactions = state['interactions']\n",
    "        self._watched = self._build_watched(self.interactions)\n",
    "        mappings = self._build_mappings(self.interactions)\n",
    "        self.e2i_user_ids, self.i2e_user_ids = mappings[0], mappings[1]\n",
    "        self.e2i_item_ids, self.i2e_item_ids = mappings[2], mappings[3]\n",
    "        self._create_index(\n",
    "            self.item_embeddings,\n",
    "            ef_construction=ef_construction,\n",
    "            n_threads=n_threads_construction,\n",
    "            ef_search=ef_search,\n",
    "            m=m,\n",
    "        )\n",
    "        # Рассчитаем средний embedding\n",
    "        self.mean_user = np.mean(self.user_embeddings, axis=0)\n",
    "\n",
    "    def predict(self, user_id: int, k_recs: int = 10) -> List[int]:\n",
    "        \"\"\"Get prediction for the given user.\n",
    "\n",
    "        Args:\n",
    "            user_id: The user ID.\n",
    "            k_recs: The number of recommendations to return.\n",
    "\n",
    "        Returns:\n",
    "            The list with the recommendations (len == k_recs).\n",
    "        \"\"\"\n",
    "        internal_user_id = self.e2i_user_ids.get(user_id)\n",
    "        if internal_user_id is not None:\n",
    "            user_embedding = self.user_embeddings[internal_user_id]\n",
    "        else:\n",
    "            # Для холодных пользователей используем среднее значение\n",
    "            user_embedding = self.mean_user\n",
    "        recs = self.index.knnQuery(user_embedding, k=100)[0]\n",
    "        recs = np.array(list(map(self.i2e_item_ids.get, recs)))\n",
    "        if internal_user_id is not None:\n",
    "            watched = np.array(self._watched.loc[user_id])\n",
    "            recs = recs[np.isin(recs, watched, invert=True)]\n",
    "        return recs[:k_recs].tolist()\n",
    "\n",
    "    def _build_watched(self, interactions: pd.DataFrame) -> pd.Series:\n",
    "        user_groups = interactions.groupby(self.user_column)\n",
    "        return user_groups[self.item_column].apply(list)\n",
    "\n",
    "    def _build_mappings(\n",
    "        self, interactions: pd.DataFrame,\n",
    "    ) -> Tuple[Dict, Dict, Dict, Dict]:\n",
    "        external_user_ids = interactions[self.user_column].sort_values(\n",
    "        ).unique()\n",
    "        external_item_ids = interactions[self.item_column].sort_values(\n",
    "        ).unique()\n",
    "        e2i_user_ids, i2e_user_ids = self._build_mapping(\n",
    "            external_user_ids,\n",
    "        )\n",
    "        e2i_item_ids, i2e_item_ids = self._build_mapping(\n",
    "            external_item_ids,\n",
    "        )\n",
    "        return e2i_user_ids, i2e_user_ids, e2i_item_ids, i2e_item_ids\n",
    "\n",
    "    def _build_mapping(self, ids: np.ndarray) -> Tuple[Dict, Dict]:\n",
    "        e2i = {e_id: i_id for i_id, e_id in enumerate(ids)}\n",
    "        i2e = {i_id: e_id for e_id, i_id in e2i.items()}\n",
    "        return e2i, i2e\n",
    "\n",
    "    def _load(\n",
    "        self, dirname: str,\n",
    "    ) -> Dict[str, Any]:\n",
    "        attr_values = {}\n",
    "        for attr_name in self.attrs_to_load:\n",
    "            attr_filename = os.path.join(\n",
    "                dirname, '{0}.pickle'.format(attr_name),\n",
    "            )\n",
    "            with open(attr_filename, 'rb') as attr_file:\n",
    "                attr_values[attr_name] = pickle.load(attr_file)\n",
    "        return attr_values\n",
    "\n",
    "    def _create_index(\n",
    "        self,\n",
    "        embeddings: np.ndarray,\n",
    "        ef_construction: int,\n",
    "        n_threads: int,\n",
    "        ef_search: int,\n",
    "        m: int,\n",
    "    ) -> None:\n",
    "        index_time_params = {\n",
    "            'M': m,\n",
    "            'indexThreadQty': n_threads,\n",
    "            'efConstruction': ef_construction,\n",
    "        }\n",
    "        aug_item_embeddings = self._aug_inner_product(embeddings)\n",
    "        self.index.addDataPointBatch(aug_item_embeddings)\n",
    "        self.index.createIndex(index_time_params)\n",
    "        query_time_params = {'efSearch': ef_search}\n",
    "        self.index.setQueryTimeParams(query_time_params)\n",
    "\n",
    "    def _aug_inner_product(self, factors: np.ndarray) -> np.ndarray:\n",
    "        normed_factors = np.linalg.norm(factors, axis=1)\n",
    "        max_norm = np.max(normed_factors)\n",
    "        extra_dim = np.sqrt(\n",
    "            max_norm ** 2 - normed_factors ** 2,\n",
    "        ).reshape(-1, 1)\n",
    "        return np.append(factors, extra_dim, axis=1)\n",
    "\n",
    "    def _aug_zero(self, factors: np.ndarray) -> np.ndarray:\n",
    "        zero = np.zeros((factors.shape[0], 1))\n",
    "        return np.append(factors, zero, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7cda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    interactions = pd.read_csv('kion_train/interactions.csv')\n",
    "    users = pd.read_csv('kion_train/users.csv')\n",
    "    items = pd.read_csv('kion_train/items.csv')\n",
    "\n",
    "    interactions.rename(\n",
    "        columns={'last_watch_dt': Columns.Datetime},\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    interactions[Columns.Datetime] = pd.to_datetime(interactions[Columns.Datetime])\n",
    "    interactions[Columns.Weight] = np.where(interactions['watched_pct'] > 10, 3, 1)\n",
    "    \n",
    "    return interactions, users, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b59e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, filter_cold_users, time_delta):\n",
    "    max_date = df[Columns.Datetime].max()\n",
    "    train = df[df[Columns.Datetime] < max_date - time_delta].copy()\n",
    "    test = df[df[Columns.Datetime] >= max_date - time_delta].copy()\n",
    "    \n",
    "    if filter_cold_users:\n",
    "        cold_users = set(test[Columns.User]) - set(train[Columns.User])\n",
    "        test.drop(test[test[Columns.User].isin(cold_users)].index, inplace=True)\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb77d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions, users, items = read_data()\n",
    "train, test = train_test_split(interactions, True, pd.Timedelta(days=7))\n",
    "dataset = Dataset.construct(interactions_df=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c392c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FmAnn('partial_best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991fe590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73c62076860412c9f5a8473af82bd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAP@10': 0.07157124369189712}\n"
     ]
    }
   ],
   "source": [
    "# Сравним значение метрики при использовании ANN\n",
    "user_ids = test[Columns.User].unique()\n",
    "\n",
    "recs, users, ranks = [], [], []\n",
    "k_recs = 10\n",
    "for user_id in tqdm(user_ids):\n",
    "    recs.extend(model.predict(user_id, k_recs=k_recs))\n",
    "    users.extend([user_id] * k_recs)\n",
    "    ranks.extend(list(range(1, k_recs + 1)))\n",
    "    \n",
    "metrics = {'MAP@10': MAP(k=10)}\n",
    "\n",
    "recos = pd.DataFrame(\n",
    "    {\n",
    "        Columns.User: users,\n",
    "        Columns.Item: recs,\n",
    "        Columns.Rank: ranks,\n",
    "    },\n",
    ")\n",
    "\n",
    "metric_values = calc_metrics(metrics, recos, test, train)\n",
    "print(metric_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
